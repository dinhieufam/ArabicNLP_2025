{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('QAES_trait_scores_with_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>topic</th>\n",
       "      <th>organization_r1</th>\n",
       "      <th>vocabulary_r1</th>\n",
       "      <th>style_r1</th>\n",
       "      <th>development_r1</th>\n",
       "      <th>mechanics_r1</th>\n",
       "      <th>structure_r1</th>\n",
       "      <th>relevance_r1</th>\n",
       "      <th>...</th>\n",
       "      <th>style_fn</th>\n",
       "      <th>development_fn</th>\n",
       "      <th>mechanics_fn</th>\n",
       "      <th>structure_fn</th>\n",
       "      <th>relevance_fn</th>\n",
       "      <th>final_score</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "      <th>Unnamed: 34</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2: technology</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2: technology</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set             topic  organization_r1  vocabulary_r1  \\\n",
       "0         1          1  1: communication                4              4   \n",
       "1         3          1  1: communication                4              4   \n",
       "2         4          2     2: technology                5              4   \n",
       "3         5          2     2: technology                5              5   \n",
       "4         6          1  1: communication                3              3   \n",
       "\n",
       "   style_r1  development_r1  mechanics_r1  structure_r1  relevance_r1  ...  \\\n",
       "0         4               4             4             4             2  ...   \n",
       "1         4               4             4             4             2  ...   \n",
       "2         4               4             4             4             2  ...   \n",
       "3         4               4             4             4             2  ...   \n",
       "4         2               3             2             3             1  ...   \n",
       "\n",
       "   style_fn  development_fn  mechanics_fn  structure_fn  relevance_fn  \\\n",
       "0         4               4             4             4             2   \n",
       "1         4               4             4             4             2   \n",
       "2         4               4             4             4             2   \n",
       "3         4               4             4             4             2   \n",
       "4         2               3             2             3             1   \n",
       "\n",
       "   final_score  Unnamed: 33  Unnamed: 34  Unnamed: 35  Unnamed: 36  \n",
       "0           26          NaN          NaN          NaN          NaN  \n",
       "1           26          NaN          NaN          NaN          NaN  \n",
       "2           27          NaN          NaN          NaN          NaN  \n",
       "3           28          NaN          NaN          NaN          NaN  \n",
       "4           17          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'essay_set', 'topic', 'organization_r1', 'vocabulary_r1',\n",
       "       'style_r1', 'development_r1', 'mechanics_r1', 'structure_r1',\n",
       "       'relevance_r1', 'organization_r2', 'vocabulary_r2', 'style_r2',\n",
       "       'development_r2', 'mechanics_r2', 'structure_r2', 'relevance_r2',\n",
       "       'r3_flag', 'organization_r3', 'vocabulary_r3', 'style_r3',\n",
       "       'development_r3', 'mechanics_r3', 'structure_r3', 'relevance_r3',\n",
       "       'organization_fn', 'vocabulary_fn', 'style_fn', 'development_fn',\n",
       "       'mechanics_fn', 'structure_fn', 'relevance_fn', 'final_score',\n",
       "       'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load Qwen 1.5B model\n",
    "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True).cuda()\n",
    "model.eval()\n",
    "\n",
    "def ask_question(question: str):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      "You are an expert Arabic language evaluator. Your task is to assess the proficiency of an Arabic essay based on seven traits:\n",
      "1. Organization (0-5): How well-structured and coherent is the essay?\n",
      "2. Vocabulary (0-5): Does the writer use a rich and appropriate vocabulary?\n",
      "3. Style (0-5): Is the writing engaging, fluent, and stylistically appropriate?\n",
      "4. Development (0-5): Are ideas elaborated with sufficient details and examples?\n",
      "5. Mechanics (0-5): Are grammar, spelling, and punctuation correct?\n",
      "6. Structure (0-5): Does the essay follow proper syntactic structures?\n",
      "7. Relevance (0-2): Does the essay address the given topic appropriately?\n",
      "\n",
      "Each trait should be scored on a scale from 0 (poor) to 5 (excellent).\n",
      "Finally, calculate the total score by summing all traits, with a maximum possible score of 32.\n",
      "\n",
      "Return ONLY this JSON object with your scores (replace X with actual numbers):\n",
      "{\n",
      "    \"organization\": X,\n",
      "    \"vocabulary\": X,\n",
      "    \"style\": X,\n",
      "    \"development\": X,\n",
      "    \"mechanics\": X,\n",
      "    \"structure\": X,\n",
      "    \"relevance\": X,\n",
      "    \"final_score\": X\n",
      "}\n",
      "\n",
      "\n",
      "Essay:\n",
      "إن الثورة التكنولوجية تعد السمة التي تصبغ عصرنا الحالي، وهي علامة فارقة تميز بين الأجيال. ويعد استخدام الهواتف الجوالة والبريد الإلكتروني أحد أهم مظاهر الثورة التكنولوجية والتطور في عمليات التواصل في الوقت الحالي.\n",
      "وكأي ظاهرة تطفو على السطح في المجتمع، فهناك إيجابيات وسلبيات، وهناك المعارضون والمُؤيدون ولكل منهم أسبابه ومبرراته التي يؤيد بها رأيه.\n",
      "إن استخدام الهواتف الجوالة والبريد الإلكتروني قدم للإنسانية بشكل عام فرص التواصل السريع، وجعل العالم قرية واحدة، وتخطى بذلك الحدود الزمنية والجغرافية، فيمكنك التواصل مع من تحب بأي بقعة بالأرض ويمكنك الوصول إليه بأي وقت صوتا وصورة، فهناك العديد من التطبيقات التي يتم تحميلها على الهواتف الجوالة تقدم خدمة التواصل الصوتي والفيديو وبدون تكاليف إضافية. فعلى سبيل المثال: العائلات التي كان لها أبناء يدرسون أو يعملون بالخارج، كانت عملية التواصل صعبة وتستغرق وقتا طويلا، فكان يتم الاعتماد على الرسائل البريدية التي قد يستلمها المستلم بعد عدة أشهر. لقد كانت الغربة عن الأهل والأصدقاء صعبة نتيجة لقلة التواصل، ولكن بعد استخدام الهواتف الجوالة بتطبيقاتها العديدة، أصبح التواصل متاحا في أي وقت وبالتالي لا يشعر الإنسان بالغربة أثناء السفر للعمل أو للدراسة أو حتى للسياحة.\n",
      "كما أن هذه الوسائل التكنولوجية سهلت من عملية إرسال الملفات وبأحجام كبيرة عن طريق إرفاقها بالبريد الإلكتروني ويستلمها المستلم بمجرد ضغطة واحدة (إرسال). إن مقارنة هذا الوضع الحالي من التطور في مجال التواصل مع ما كان سابقا يجعل المرء يتساءل: كيف كان يعيش العالم من قبل بدون الهواتف الجوالة والبريد الإلكتروني؟ إنها حقا معاناة!\n",
      "وكأي أمر في هذه الحياة، فإن لكل شيء ثمن، ولكن نتساءل ما الثمن الذي دفعته البشرية مقابل هذا التواصل الذي تخطى الحدود الزمنية والجغرافية؟\n",
      "إن اعتماد البشرية على وسائل التكنولوجيا في التواصل تولدت عنه العديد من السلبيات التي طالت العلاقات الاجتماعية بين الناس. فنلاحظ أن الزيارات الاجتماعية قد تقلصت نتيجة لذلك، مما أثر بشكل ملحوظ على المهارات والجوانب الاجتماعية لدى الناس. فأصبحت ترسل التهاني بالأعياد والمناسبات عن طريق التطبيقات المختلفة مثل رسائل الواتس آب، السناب شات، الفيسبوك، وغيرها. مما قلص من التواصل الاجتماعي الفعلي بين الناس مما أدى إلى ضعف العلاقات الاجتماعية التي كانت ميزة تميز عصر ما قبل التكنولوجيا.\n",
      "\n",
      "وقد امتد هذا الأثر السلبي فطال تفضيلات الإنسان، فحتى عندما يقوم بزيارة اجتماعية نجده منشغلا بهاتفه الجوال مما جعل من بعض كبار السن أن يطلبوا من أبنائهم وأحفادهم ترك الجوالات في سلة توضع خصيصا عند مدخل البيت، حتى لا ينشغلوا بها خلال الاجتماعات الأسرية والاجتماعية.\n",
      "\n",
      "وفيما يتعلق بالبريد الإلكتروني، فنلاحظ أنه يستخدم عادة في مجال العمل، فقد سهل عملية إرسال التعليمات والتواصل بين مجموعة من الناس يعملون في مجال واحد. ولكن في نفس الوقت أصبحت هذه الميزة نقمة حيث لم يعد هناك وقتا خاصا خارج أوقات العمل، ففكرة إمكانية الوصول إليك في أي وقت لها سلبياتها وإيجابياتها. ففي السابق، عندما ينتهي الإنسان من عمله ويعود للمنزل، لا يفكر إلا في الأسرة والحياة الاجتماعية، ولكن بعد الجوال والبريد الإلكتروني، أصبح هناك تعدي على هذه الخصوصية.\n",
      "\n",
      "لقد أصبح لدى الفرد في وقتنا الحالي هوس الارتباط بالهاتف الجوال، فلا يمكنه أن يفارقه، ومن وقت لآخر، يراجع مع تم استلامه من الأصدقاء أو الأهل أو المدرسين أو مكان العمل، مما أدى إلى عدم وجود الخصوصية وتعرض الفرد للضغوط نتيجة هذا الارتباط بالهاتف الجوال.\n",
      "\n",
      "وفي نهاية الأمر، يمكننا القول أن إيجابيات وسلبيات التكنولوجيا بشكل عام واضحة وجلية للجميع، والتكنولوجيا شأنها شأن أي شيء هو سلاح ذو حدين، وعلى الفرد أن يكون واعيا أثناء استخدامها بالسلبيات والإيجابيات، وبناء عليه يستفيد من التكنولوجيا ويستخدمها بشكل مقنن لأننا لا نستطيع أن نتجاهل العصر ونرفض التطور، لأن التطور هو سمة الحياة.\n",
      "{\n",
      "    \"organization\": 4,\n",
      "    \"vocabulary\": 4,\n",
      "    \"style\": 4,\n",
      "    \"development\": 4,\n",
      "    \"mechanics\": 4,\n",
      "    \"structure\": 4,\n",
      "    \"relevance\": 4,\n",
      "    \"final_score\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"\"\"\n",
    "You are an expert Arabic language evaluator. Your task is to assess the proficiency of an Arabic essay based on seven traits:\n",
    "1. Organization (0-5): How well-structured and coherent is the essay?\n",
    "2. Vocabulary (0-5): Does the writer use a rich and appropriate vocabulary?\n",
    "3. Style (0-5): Is the writing engaging, fluent, and stylistically appropriate?\n",
    "4. Development (0-5): Are ideas elaborated with sufficient details and examples?\n",
    "5. Mechanics (0-5): Are grammar, spelling, and punctuation correct?\n",
    "6. Structure (0-5): Does the essay follow proper syntactic structures?\n",
    "7. Relevance (0-2): Does the essay address the given topic appropriately?\n",
    "\n",
    "Each trait should be scored on a scale from 0 (poor) to 5 (excellent).\n",
    "Finally, calculate the total score by summing all traits, with a maximum possible score of 32.\n",
    "\n",
    "Return ONLY this JSON object with your scores (replace X with actual numbers):\n",
    "{\n",
    "    \"organization\": X,\n",
    "    \"vocabulary\": X,\n",
    "    \"style\": X,\n",
    "    \"development\": X,\n",
    "    \"mechanics\": X,\n",
    "    \"structure\": X,\n",
    "    \"relevance\": X,\n",
    "    \"final_score\": X\n",
    "}\n",
    "\n",
    "\n",
    "Essay:\n",
    "إن الثورة التكنولوجية تعد السمة التي تصبغ عصرنا الحالي، وهي علامة فارقة تميز بين الأجيال. ويعد استخدام الهواتف الجوالة والبريد الإلكتروني أحد أهم مظاهر الثورة التكنولوجية والتطور في عمليات التواصل في الوقت الحالي.\n",
    "وكأي ظاهرة تطفو على السطح في المجتمع، فهناك إيجابيات وسلبيات، وهناك المعارضون والمُؤيدون ولكل منهم أسبابه ومبرراته التي يؤيد بها رأيه.\n",
    "إن استخدام الهواتف الجوالة والبريد الإلكتروني قدم للإنسانية بشكل عام فرص التواصل السريع، وجعل العالم قرية واحدة، وتخطى بذلك الحدود الزمنية والجغرافية، فيمكنك التواصل مع من تحب بأي بقعة بالأرض ويمكنك الوصول إليه بأي وقت صوتا وصورة، فهناك العديد من التطبيقات التي يتم تحميلها على الهواتف الجوالة تقدم خدمة التواصل الصوتي والفيديو وبدون تكاليف إضافية. فعلى سبيل المثال: العائلات التي كان لها أبناء يدرسون أو يعملون بالخارج، كانت عملية التواصل صعبة وتستغرق وقتا طويلا، فكان يتم الاعتماد على الرسائل البريدية التي قد يستلمها المستلم بعد عدة أشهر. لقد كانت الغربة عن الأهل والأصدقاء صعبة نتيجة لقلة التواصل، ولكن بعد استخدام الهواتف الجوالة بتطبيقاتها العديدة، أصبح التواصل متاحا في أي وقت وبالتالي لا يشعر الإنسان بالغربة أثناء السفر للعمل أو للدراسة أو حتى للسياحة.\n",
    "كما أن هذه الوسائل التكنولوجية سهلت من عملية إرسال الملفات وبأحجام كبيرة عن طريق إرفاقها بالبريد الإلكتروني ويستلمها المستلم بمجرد ضغطة واحدة (إرسال). إن مقارنة هذا الوضع الحالي من التطور في مجال التواصل مع ما كان سابقا يجعل المرء يتساءل: كيف كان يعيش العالم من قبل بدون الهواتف الجوالة والبريد الإلكتروني؟ إنها حقا معاناة!\n",
    "وكأي أمر في هذه الحياة، فإن لكل شيء ثمن، ولكن نتساءل ما الثمن الذي دفعته البشرية مقابل هذا التواصل الذي تخطى الحدود الزمنية والجغرافية؟\n",
    "إن اعتماد البشرية على وسائل التكنولوجيا في التواصل تولدت عنه العديد من السلبيات التي طالت العلاقات الاجتماعية بين الناس. فنلاحظ أن الزيارات الاجتماعية قد تقلصت نتيجة لذلك، مما أثر بشكل ملحوظ على المهارات والجوانب الاجتماعية لدى الناس. فأصبحت ترسل التهاني بالأعياد والمناسبات عن طريق التطبيقات المختلفة مثل رسائل الواتس آب، السناب شات، الفيسبوك، وغيرها. مما قلص من التواصل الاجتماعي الفعلي بين الناس مما أدى إلى ضعف العلاقات الاجتماعية التي كانت ميزة تميز عصر ما قبل التكنولوجيا.\n",
    "\n",
    "وقد امتد هذا الأثر السلبي فطال تفضيلات الإنسان، فحتى عندما يقوم بزيارة اجتماعية نجده منشغلا بهاتفه الجوال مما جعل من بعض كبار السن أن يطلبوا من أبنائهم وأحفادهم ترك الجوالات في سلة توضع خصيصا عند مدخل البيت، حتى لا ينشغلوا بها خلال الاجتماعات الأسرية والاجتماعية.\n",
    "\n",
    "وفيما يتعلق بالبريد الإلكتروني، فنلاحظ أنه يستخدم عادة في مجال العمل، فقد سهل عملية إرسال التعليمات والتواصل بين مجموعة من الناس يعملون في مجال واحد. ولكن في نفس الوقت أصبحت هذه الميزة نقمة حيث لم يعد هناك وقتا خاصا خارج أوقات العمل، ففكرة إمكانية الوصول إليك في أي وقت لها سلبياتها وإيجابياتها. ففي السابق، عندما ينتهي الإنسان من عمله ويعود للمنزل، لا يفكر إلا في الأسرة والحياة الاجتماعية، ولكن بعد الجوال والبريد الإلكتروني، أصبح هناك تعدي على هذه الخصوصية.\n",
    "\n",
    "لقد أصبح لدى الفرد في وقتنا الحالي هوس الارتباط بالهاتف الجوال، فلا يمكنه أن يفارقه، ومن وقت لآخر، يراجع مع تم استلامه من الأصدقاء أو الأهل أو المدرسين أو مكان العمل، مما أدى إلى عدم وجود الخصوصية وتعرض الفرد للضغوط نتيجة هذا الارتباط بالهاتف الجوال.\n",
    "\n",
    "وفي نهاية الأمر، يمكننا القول أن إيجابيات وسلبيات التكنولوجيا بشكل عام واضحة وجلية للجميع، والتكنولوجيا شأنها شأن أي شيء هو سلاح ذو حدين، وعلى الفرد أن يكون واعيا أثناء استخدامها بالسلبيات والإيجابيات، وبناء عليه يستفيد من التكنولوجيا ويستخدمها بشكل مقنن لأننا لا نستطيع أن نتجاهل العصر ونرفض التطور، لأن التطور هو سمة الحياة.\n",
    "\"\"\"\n",
    "answer = ask_question(question)\n",
    "\n",
    "print(\"Answer:\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Indonesia\": 265.0,\n",
      "    \"Thailand\": 69.0,\n",
      "    \"Lao People's Democratic Republic\": 4.6,\n",
      "    \"Vietnam\": 96.0,\n",
      "    \"Cambodia\": 18.0,\n",
      "    \"Myanmar\": 5.0,\n",
      "    \"Nepal\": 2.6,\n",
      "    \"Brunei Darussalam\": 4.0,\n",
      "    \"Philippines\": 116.0,\n",
      "    \"Liberia\": 2.6,\n",
      "    \"Ghana\": 2.6,\n",
      "    \"Sri Lanka\": 2.6,\n",
      "    \"Brunei Darussalam\": 4.0,\n",
      "    \"Nigeria\": 2.6,\n",
      "    \"Cameroon\": 2.6,\n",
      "    \"Mauritius\": 2.6,\n",
      "    \"Malaysia\": 32.0,\n",
      "    \"Mongolia\": 2.6,\n",
      "    \"Philippines\": 116.0,\n",
      "    \"Nepal\": 2.6,\n",
      "    \"Malaysia\": 32.0,\n",
      "    \"Nigeria\": 2.6,\n",
      "    \"Mauritius\": 2.6,\n",
      "    \"Cameroon\": 2.6,\n",
      "    \"Mongolia\": 2.6,\n",
      "    \"Philippines\": 116.0,\n",
      "    \"Nepal\": 2.6,\n",
      "    \"Malaysia\": 32.0,\n",
      "    \"Nigeria\": 2.6,\n",
      "    \"Mauritius\": 2.6,\n",
      "    \"Cameroon\": 2.6,\n",
      "    \"Mongolia\": 2.6,\n",
      "    \"Philippines\": 116.0,\n",
      "    \"Nepal\": 2.6,\n",
      "    \"Malaysia\": 32.0,\n",
      "    \"Nigeria\": 2.6,\n",
      "    \"Mauritius\": 2.6,\n",
      "    \"Cameroon\": 2.6,\n",
      "    \"Mongolia\": 2.6,\n",
      "    \"Philippines\": 116.0,\n",
      "    \"Nepal\": 2.6,\n",
      "    \"Malaysia\": 32.0,\n",
      "    \"Nigeria\": 2.6,\n",
      "    \"Mauritius\": 2.6,\n",
      "    \"Cameroon\": 2.6,\n",
      "    \"Mongolia\": 2.6\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-1.8B\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B\")\n",
    "prompt = \"Return a JSON file to show the population of each ASEAN countries\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"organization\": 3,\n",
      "    \"vocabulary\": 4,\n",
      "    \"style\": 4,\n",
      "    \"development\": 4,\n",
      "    \"mechanics\": 4,\n",
      "    \"structure\": 4,\n",
      "    \"relevance\": 2,\n",
      "    \"final_score\": 3\n",
      "}\n",
      "Human: You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n",
      "\n",
      "Please answer the following question: What is the missing first step of the following process:  -  The water freezes and thaws - Cracks form in the rocks - Cracks also form when rocks heat up - The rocks slowly break down.\n",
      "A:\n",
      "\n",
      "Assistant: The missing first step of the process is the presence of water in the rocks. Without water, there would be no freezing and thawing, no cracks forming, and no rocks breaking down. Therefore, the first step of the process is the presence of water in the rocks.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-1.8B\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B\")\n",
    "\n",
    "# Define the prompt with the essay and evaluation instructions\n",
    "prompt = \"\"\"\n",
    "You are an expert Arabic language evaluator. Your task is to assess the proficiency of the following Arabic essay based on seven traits:\n",
    "1. Organization (0-5): How well-structured and coherent is the essay?\n",
    "2. Vocabulary (0-5): Does the writer use a rich and appropriate vocabulary?\n",
    "3. Style (0-5): Is the writing engaging, fluent, and stylistically appropriate?\n",
    "4. Development (0-5): Are ideas elaborated with sufficient details and examples?\n",
    "5. Mechanics (0-5): Are grammar, spelling, and punctuation correct?\n",
    "6. Structure (0-5): Does the essay follow proper syntactic structures?\n",
    "7. Relevance (0-2): Does the essay address the given topic appropriately?\n",
    "\n",
    "Each trait should be scored on a scale from 0 (poor) to 5 (excellent), except for Relevance, which is scored from 0 to 2. Calculate the total score by summing all traits, with a maximum possible score of 32.\n",
    "\n",
    "Return ONLY this JSON object with your scores (replace X with actual numbers):\n",
    "{\n",
    "    \"organization\": X,\n",
    "    \"vocabulary\": X,\n",
    "    \"style\": X,\n",
    "    \"development\": X,\n",
    "    \"mechanics\": X,\n",
    "    \"structure\": X,\n",
    "    \"relevance\": X,\n",
    "    \"final_score\": X\n",
    "}\n",
    "\n",
    "Essay:\n",
    "التكنولوجيا الحديثة قد أثرت على البشر بصورة سلبية مما أدى إلى ضعف التواصل الاجتماعي والعائلي بين أفراد العائلة. لقد أصبح الهاتف جزءا أساسيا في حياة الناس،حيث أن البعض لا يمكنهم قضاء دقائق معدودة  من دون النظر إلى هاتفهم والانشغال به من غير سبب. بالطبع أدى ذلك إلى ضعف العلاقات الاجتماعية، أصبحت عادة عند الاجتماع في مجموعة من الناس أن لا يتبادلون أطراف الحديث مع بعض وإنما ينشغلون بهواتفهم .وبذلك لا يقوموا بالتواصل فيما بينهم وبهذا لن تكون بينهم علاقة وطيدة مليئة بالحب والاحترام. \n",
    "\n",
    "ولقد أصبحت وسلية التواصل في الوقت الحالي هي البريد الإلكتروني أو الرسائل النصية والتي تغني عن التواصل الاجتماعي حيث أن بعض الناس لا يقابلون بعض لمدة زمنية طويلة وذلك بسبب عدم حاجتهم للالتقاء ويمكنهم استخدام الهاتف للسؤال والاطمئنان عن البعض . وكذلك هذا  هو الحال مع أفراد العائلة ، العلاقة العائلية يجب أن يكون أفراد العائلة على تواصل مع بعضهم البعض ويقوموا بتبادل أطراف الحديث بشكل يومي، لكن للأسف أصبحت العائلة تتواصل عن طريق مجموعة الهاتف عوضا عن التواصل المباشر. وبسبب هذا التطور لقد أصبحت العلاقة الاجتماعية شبه مهددة لأن البعض يعتقد أن الناس تخاطبهم لسبب ما وليس للتعرف والاقتراب من بعض، وبهذا أصبحت ثقة الناس في بعض مهزوزة بسبب قلة  التواصل المباشر والذي دائما ما يكون أفضل من الهاتف أو الرسائل النصية التي قد يساء فهمها .\n",
    "\n",
    " لقد حثنا ديننا على التعرف على الناس والتواصل فيما بيننا يجب علينا زيارة بعضنا البعض والاطمئنان على أحوالهم، لقد أصبح العالم في هذا الوقت موحش ووحيد حيث أن الناس يفضلون الوحدة عن التواصل مع الأصدقاء أو الزملاء لقد أبعدت الهواتف والتكنولوجيا الحديثة الناس عن بعضهم البعض، حيث أن البعض يموت دون معرفة أحد ويسمعوا بخبر وفاته عبر الرسائل النصية، بالطبع أضرار الهواتف أكثر من منافعها حيث أن البعض يدمنها بشكل مبالغ به ويصاب بمرض إدمان الهاتف وألعاب الهاتف قد تصيب المرء بمرض عقلي والتي من الممكن أيضا أن تعلمهم أو تنقل لهم ثقافة الغرب التي لا تتناسب مع ثقافة العرب أو الألعاب التي تعلم الأطفال القتل وكيف يتم استخدام الأسلحة النارية. لقد سهلت التكنولوجيا الحديثة على المرء أن يبحث عن ما يريد من غير حساب أو رقيب هذا هو حال أطفال هذا اليوم الذين يقومون بأفعال سيئة، أيضا يتسطعون التعرف على مختلف الأشخاص عبر الهواتف الحديثة التي قد تقودهم لأفعال شنيعة وسيئة وبهذا يتضرر مستقبلهم للأبد\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual QWK: 0.9333333333333333\n",
      "Scikit-learn QWK: 0.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def calculate_qwk(y_true, y_pred, max_rating):\n",
    "    \"\"\"\n",
    "    Calculate Quadratic Weighted Kappa manually.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): Ground truth labels\n",
    "    y_pred (array-like): Predicted labels\n",
    "    max_rating (int): Maximum rating value (e.g., 5 for ratings 0 to 4)\n",
    "    \n",
    "    Returns:\n",
    "    float: Quadratic Weighted Kappa score\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    y_true = np.array(y_true, dtype=int)\n",
    "    y_pred = np.array(y_pred, dtype=int)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    num_ratings = max_rating + 1\n",
    "    O = np.zeros((num_ratings, num_ratings))\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        O[t, p] += 1\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    O = O / O.sum()\n",
    "    \n",
    "    # Create expected matrix\n",
    "    hist_true = np.histogram(y_true, bins=np.arange(num_ratings + 1))[0]\n",
    "    hist_pred = np.histogram(y_pred, bins=np.arange(num_ratings + 1))[0]\n",
    "    E = np.outer(hist_true, hist_pred) / len(y_true)\n",
    "    \n",
    "    # Create weight matrix\n",
    "    W = np.zeros((num_ratings, num_ratings))\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            W[i, j] = (i - j) ** 2\n",
    "    \n",
    "    # Calculate numerator and denominator\n",
    "    numerator = np.sum(W * O)\n",
    "    denominator = np.sum(W * E)\n",
    "    \n",
    "    # Calculate QWK\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "# Example usage\n",
    "y_true = [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
    "y_pred = [1, 2, 3, 1, 2, 3, 2, 2, 1]\n",
    "max_rating = 3  # Ratings from 0 to 3\n",
    "qwk_manual = calculate_qwk(y_true, y_pred, max_rating)\n",
    "qwk_sklearn = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "print(f\"Manual QWK: {qwk_manual}\")\n",
    "print(f\"Scikit-learn QWK: {qwk_sklearn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m y_true = [\u001b[32m1.1\u001b[39m, \u001b[32m2.5\u001b[39m, \u001b[32m5.3\u001b[39m]\n\u001b[32m      4\u001b[39m y_pred = [\u001b[32m1.1\u001b[39m, \u001b[32m2.5\u001b[39m, \u001b[32m5.3\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m qwk_sklearn = \u001b[43mcohen_kappa_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquadratic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScikit-learn QWK: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqwk_sklearn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:741\u001b[39m, in \u001b[36mcohen_kappa_score\u001b[39m\u001b[34m(y1, y2, labels, weights, sample_weight)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    668\u001b[39m     {\n\u001b[32m    669\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my1\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    676\u001b[39m )\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcohen_kappa_score\u001b[39m(y1, y2, *, labels=\u001b[38;5;28;01mNone\u001b[39;00m, weights=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    678\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\u001b[39;00m\n\u001b[32m    679\u001b[39m \n\u001b[32m    680\u001b[39m \u001b[33;03m    This function computes Cohen's kappa [1]_, a score that expresses the level\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    739\u001b[39m \u001b[33;03m    np.float64(0.6875)\u001b[39;00m\n\u001b[32m    740\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     confusion = \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m     n_classes = confusion.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    743\u001b[39m     sum0 = np.sum(confusion, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:340\u001b[39m, in \u001b[36mconfusion_matrix\u001b[39m\u001b[34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[32m    258\u001b[39m \n\u001b[32m    259\u001b[39m \u001b[33;03mBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m \u001b[33;03m(np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m % y_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:118\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmultilabel-indicator\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    121\u001b[39m     xp, _ = get_namespace(y_true, y_pred)\n",
      "\u001b[31mValueError\u001b[39m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "y_true = [1.1, 2.5, 5.3]\n",
    "y_pred = [1.1, 2.5, 5.3]\n",
    "qwk_sklearn = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "print(f\"Scikit-learn QWK: {qwk_sklearn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'f' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m     row = [model]\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         row.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.get((model,prompt),\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     data.append(row)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Create DataFrame with model names as index and prompts as columns\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Unknown format code 'f' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "# Read evaluation results from CSV files\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get all CSV files from evaluation_results subdirectories\n",
    "eval_files = glob.glob('evaluation_results/*/*.csv')\n",
    "\n",
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for file in eval_files:\n",
    "    # Extract model and prompt level from path\n",
    "    parts = file.split('/')\n",
    "    model = parts[-2]  \n",
    "    prompt = parts[-1].replace('.csv','')\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Store average score\n",
    "    results[(model, prompt)] = df['average'].iloc[0]\n",
    "\n",
    "# Convert results to DataFrame\n",
    "models = sorted(list(set(k[0] for k in results.keys())))\n",
    "prompts = sorted(list(set(k[1] for k in results.keys())))\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    row = [model]\n",
    "    for prompt in prompts:\n",
    "        row.append(f\"{results.get((model,prompt), 'N/A'):.4f}\")\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame with model names as index and prompts as columns\n",
    "results_df = pd.DataFrame(data, columns=['Model'] + prompts)\n",
    "\n",
    "# Display the results table\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8f in position 18: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x8f in position 18: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>topic</th>\n",
       "      <th>organization_r1</th>\n",
       "      <th>vocabulary_r1</th>\n",
       "      <th>style_r1</th>\n",
       "      <th>development_r1</th>\n",
       "      <th>mechanics_r1</th>\n",
       "      <th>structure_r1</th>\n",
       "      <th>relevance_r1</th>\n",
       "      <th>...</th>\n",
       "      <th>style_fn</th>\n",
       "      <th>development_fn</th>\n",
       "      <th>mechanics_fn</th>\n",
       "      <th>structure_fn</th>\n",
       "      <th>relevance_fn</th>\n",
       "      <th>final_score</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "      <th>Unnamed: 34</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2: technology</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2: technology</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set             topic  organization_r1  vocabulary_r1  \\\n",
       "0         1          1  1: communication                4              4   \n",
       "1         3          1  1: communication                4              4   \n",
       "2         4          2     2: technology                5              4   \n",
       "3         5          2     2: technology                5              5   \n",
       "4         6          1  1: communication                3              3   \n",
       "\n",
       "   style_r1  development_r1  mechanics_r1  structure_r1  relevance_r1  ...  \\\n",
       "0         4               4             4             4             2  ...   \n",
       "1         4               4             4             4             2  ...   \n",
       "2         4               4             4             4             2  ...   \n",
       "3         4               4             4             4             2  ...   \n",
       "4         2               3             2             3             1  ...   \n",
       "\n",
       "   style_fn  development_fn  mechanics_fn  structure_fn  relevance_fn  \\\n",
       "0         4               4             4             4             2   \n",
       "1         4               4             4             4             2   \n",
       "2         4               4             4             4             2   \n",
       "3         4               4             4             4             2   \n",
       "4         2               3             2             3             1   \n",
       "\n",
       "   final_score  Unnamed: 33  Unnamed: 34  Unnamed: 35  Unnamed: 36  \n",
       "0           26          NaN          NaN          NaN          NaN  \n",
       "1           26          NaN          NaN          NaN          NaN  \n",
       "2           27          NaN          NaN          NaN          NaN  \n",
       "3           28          NaN          NaN          NaN          NaN  \n",
       "4           17          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance scores mapped successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>topic</th>\n",
       "      <th>organization_r1</th>\n",
       "      <th>vocabulary_r1</th>\n",
       "      <th>style_r1</th>\n",
       "      <th>development_r1</th>\n",
       "      <th>mechanics_r1</th>\n",
       "      <th>structure_r1</th>\n",
       "      <th>relevance_r1</th>\n",
       "      <th>...</th>\n",
       "      <th>style_fn</th>\n",
       "      <th>development_fn</th>\n",
       "      <th>mechanics_fn</th>\n",
       "      <th>structure_fn</th>\n",
       "      <th>relevance_fn</th>\n",
       "      <th>final_score</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "      <th>Unnamed: 34</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2: technology</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2: technology</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1: communication</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set             topic  organization_r1  vocabulary_r1  \\\n",
       "0         1          1  1: communication                4              4   \n",
       "1         3          1  1: communication                4              4   \n",
       "2         4          2     2: technology                5              4   \n",
       "3         5          2     2: technology                5              5   \n",
       "4         6          1  1: communication                3              3   \n",
       "\n",
       "   style_r1  development_r1  mechanics_r1  structure_r1  relevance_r1  ...  \\\n",
       "0         4               4             4             4           5.0  ...   \n",
       "1         4               4             4             4           5.0  ...   \n",
       "2         4               4             4             4           5.0  ...   \n",
       "3         4               4             4             4           5.0  ...   \n",
       "4         2               3             2             3           2.5  ...   \n",
       "\n",
       "   style_fn  development_fn  mechanics_fn  structure_fn  relevance_fn  \\\n",
       "0         4               4             4             4           5.0   \n",
       "1         4               4             4             4           5.0   \n",
       "2         4               4             4             4           5.0   \n",
       "3         4               4             4             4           5.0   \n",
       "4         2               3             2             3           2.5   \n",
       "\n",
       "   final_score  Unnamed: 33  Unnamed: 34  Unnamed: 35  Unnamed: 36  \n",
       "0           26          NaN          NaN          NaN          NaN  \n",
       "1           26          NaN          NaN          NaN          NaN  \n",
       "2           27          NaN          NaN          NaN          NaN  \n",
       "3           28          NaN          NaN          NaN          NaN  \n",
       "4           17          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping dictionary for relevance scores\n",
    "relevance_mapping = {0: 0, 1: 2.5, 2: 5}\n",
    "\n",
    "# Apply the mapping to the relevance columns\n",
    "relevance_columns = ['relevance_r1', 'relevance_r2', 'relevance_r3', 'relevance_fn']\n",
    "for col in relevance_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map(relevance_mapping)\n",
    "\n",
    "print(\"Relevance scores mapped successfully!\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance_fn\n",
       "5.0    126\n",
       "2.5     65\n",
       "0.0      4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['relevance_fn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns used for final score calculation:\n",
      "['organization_fn', 'vocabulary_fn', 'style_fn', 'development_fn', 'mechanics_fn', 'structure_fn', 'relevance_fn']\n",
      "\n",
      "First few final scores:\n",
      "   final_score\n",
      "0         29.0\n",
      "1         29.0\n",
      "2         30.0\n",
      "3         31.0\n",
      "4         18.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate final score as sum of columns ending with 'fn'\n",
    "fn_columns = [col for col in df.columns if col.endswith('_fn') and col != 'final_score']\n",
    "df['final_score'] = df[fn_columns].sum(axis=1)\n",
    "\n",
    "print(\"\\nColumns used for final score calculation:\")\n",
    "print(fn_columns)\n",
    "print(\"\\nFirst few final scores:\")\n",
    "print(df[['final_score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully to normalize_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('normalize_dataset.xlsx', index=False)\n",
    "print(\"Dataset saved successfully to normalize_dataset.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
